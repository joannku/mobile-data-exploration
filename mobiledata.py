# -*- coding: utf-8 -*-
"""mobiledata

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jA5mo8AFEoyDzwXqLBZGtmw8HFdyfKfE

# Mobile phone health data analysis

This is a dataset composed of mobile phone activity data (Google Health Kit and Apple Health data). The following types of data have been collected, together with a recording start time, and value: active energy burned, heart rate, distance walking/running, steps, move minutes, distance travelled, flights climbed, resting heart rate, active heart rate, sleep time.
"""

# Mounting the drive

from google.colab import drive
drive.mount('/content/stanford')

!ls /content/stanford/MyDrive/stanford

# Commented out IPython magic to ensure Python compatibility.
# Loading packages

# %matplotlib inline
from datetime import datetime, time, timedelta
from scipy.cluster import hierarchy
from sklearn import cluster, preprocessing, decomposition
import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import os, sys, pdb, time, datetime as dt

# Convert the data to a pandas dataframe
df = pd.read_csv('/content/stanford/MyDrive/stanford/tracking.csv')

# This set of functions adds local start and end time 
# in datetime format
# and activity time in timedelta format

df['start_dt_utc'] = pd.to_datetime(df.start, unit = 's')
df['end_dt_utc'] = pd.to_datetime(df.end, unit = 's')
df['start_dt_local'] = pd.to_datetime(df.start + df.utc_offset * 60, unit='s')
df['end_dt_local'] = pd.to_datetime(df.end + df.utc_offset * 60, unit='s')
df['activity_time'] = df['end_dt_utc'] - df['start_dt_utc']

df['start_date'] = df['start_dt_local'].dt.date
df['start_hour'] = df['start_dt_local'].dt.hour
df['start_minute'] = df['start_dt_local'].dt.minute
df['start_second'] = df['start_dt_local'].dt.second

df['end_date'] = df['end_dt_local'].dt.date
df['end_hour'] = df['end_dt_local'].dt.hour
df['end_minute'] = df['end_dt_local'].dt.minute
df['end_second'] = df['end_dt_local'].dt.second

# View dataframe
df

# View data types
df.dtypes

# This function is to add a column with "actual start date"
# which can be useful when working with sleep data.
# Sleep after midnight should still be counted as sleep
# for the previoius day

# 1. Extract date and time from the data column 
# column is just a pandas name for a data array
# the key to the column that contains date and time
key = df['start_dt_local']
key = pd.to_datetime(key)
df['key'] = pd.to_datetime(key) # this function is really smart

# df[key] is the column, df[key].dt extracts the datetime object
date, time = df['key'].dt.date, df['key'].dt.time

# Writing a new column of data with adjusted date
# make a boolean array of when the time is before or at 7 am
mask = time <= dt.time(3, 0, 0) 
# write a new column with an adjusted date
new_key = "actual_start_date"
df[new_key] = date # make a copy of the date

# we now need to modify the new values in place, with our mask
# df.loc[:, key] is the notation for both:
# (i) seeing the whole dataframe as a matrix with vertical dimension being 
# data and horizontal being column names and
# (ii) modifying values in place
# let's adjust the dates in place

df.loc[mask, new_key] += dt.timedelta(days=-1)

# df[new_key][mask] would return a read-only view of our arrays
# so >>> df[new_key][mask] += dt.timedelta(days=-1) is illegal
# that's an odd behavior, but it's convention that df[new_key] is read-only

df

# Create new column with actual start time date

df['actual_start_date'] = pd.to_datetime(df.actual_start_date)
df

# View data types to check if correct

df.dtypes

# List all data types to ensure that the relevant ones are included in further steps

df.type.unique()

# List all user names to ensure they are included in further steps

df.userId.unique()

df_steps_date = (df
 #.set_index('userId')
 .loc[lambda df: df['type'] == 'STEPS']
 .groupby(['userId', pd.Grouper(key='start_date')])['value'].sum()
 #.agg({'value': 'sum'})
 .reset_index())

# This plot shows the number of steps per day per user
# The purpose of this is to visualise the date ranges
# and number of steps to see if data looks reasonable

with sns.plotting_context('talk'):
      fig, ax = plt.subplots(figsize=(10,8))
      steps_barplot = sns.barplot(x = 'start_date', y = 'value', data = df_steps_date, hue='userId')
      ax.tick_params(axis='x', labelrotation=45)
      ax.set(xlabel='Date', ylabel='Number of steps')
      plt.legend(loc = 2, bbox_to_anchor = (1,1))

for index, label in enumerate(steps_barplot.get_xticklabels()):
   if index % 4 == 0:
      label.set_visible(True)
   else:
      label.set_visible(False)

def change_width(ax, new_value) :
    for patch in ax.patches :
        current_width = patch.get_width()
        diff = current_width - new_value

        # we change the bar width
        patch.set_width(new_value)

        # we recenter the bar
        patch.set_x(patch.get_x() + diff * .5)

change_width(ax, .35)
plt.show()

# Create dfs for all of the metrics where daily sum is applicable

df_sleep = (df
            .loc[lambda df: df['type'] == 'SLEEP_ASLEEP']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'sleep_mins'}))

df_steps = (df
            .loc[lambda df: df['type'] == 'STEPS']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'steps'}))
            
df_kcal = (df
            .loc[lambda df: df['type'] == 'ACTIVE_ENERGY_BURNED']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'kcal'}))    

df_flights_climbed = (df
            .loc[lambda df: df['type'] == 'FLIGHTS_CLIMBED']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'flights_climbed'}))

df_distance_walking_running = (df
            .loc[lambda df: df['type'] == 'DISTANCE_WALKING_RUNNING']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'distance_walking_running'}))

df_move_time = (df
            .loc[lambda df: df['type'] == 'MOVE_MINUTES']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'move_mins'}))

df_distance_delta = (df
            .loc[lambda df: df['type'] == 'DISTANCE_DELTA']
            .set_index('actual_start_date')
            .groupby('userId').resample('d').agg({'value':'sum'})
            .rename(columns={'value':'distance_delta'}))

# Resting state HR & walking HR measurements are taken daily
# Probably this is the daily average?

df_resting_HR = (df
            .loc[lambda df: df['type'] == 'RESTING_HEART_RATE']
            .set_index('actual_start_date')
            .groupby('userId')
            .resample('d')
            .agg({'value':'mean'})
            .rename(columns={'value':'resting_HR'})
            .dropna())

df_walking_HR = (df
            .loc[lambda df: df['type'] == 'WALKING_HEART_RATE']
            .set_index('actual_start_date')
            .groupby('userId')
            .resample('d')
            .agg({'value':'mean'})
            .rename(columns={'value':'walking_HR'})
            .dropna())

# This HR metric is different because the sampling rate is 5min 

df_HR = (df
            .loc[lambda df: df['type'] == 'HEART_RATE']
            .set_index('start_dt_local')
            .groupby('userId')
            .resample('5Min')
            .agg({'value':'mean'})
            .rename(columns={'value':'HR'}))
            #.dropna())

pd.set_option('display.max_rows', 20)

df_HR_reset = df_HR.reset_index()
df_HR_reset

df.type.value_counts()

# New cleaned dataset with all data sorted by user ID and actual start date of activity 

activities = pd.concat([df_sleep, df_steps, df_kcal, df_flights_climbed, df_distance_walking_running, df_move_time, df_distance_delta, df_resting_HR, df_walking_HR], axis=1)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
activities

activities.index.levels[0]

# This is to check for missing data
activities.isnull().sum()

# Correlations between data streams 
 
activities.corr()

activities_correlation_matrix = (activities
 .corr(method='spearman')
 .style.background_gradient(cmap='viridis')
)

# Correlations matrix between data streams 
# This helps with visualising the lowest and highest values

fig, ax = plt.subplots(figsize=(8,8))
sns.heatmap(activities.corr(), cmap='viridis', vmin=-1, vmax=1, annot=True, square=True, ax=ax)

"""# PCA"""

activities_nonan = activities.fillna(1)

std = preprocessing.StandardScaler()
std_X = pd.DataFrame(std.fit_transform(activities_nonan), columns=activities_nonan.columns)
std_X

std_X.std()

std.var_

std.mean_

std_X.mean()

std.transform(activities_nonan)

std_X.steps.hist()

pca = decomposition.PCA()
pca_X = pd.DataFrame(pca.fit_transform(std_X), 
                     columns=[f'PC{i+1}' for i in range(activities_nonan.shape[1])])
pca_X

pca.explained_variance_ratio_#.cumsum()

# This plot shows that 2 first PCA components are relevant to the model
plt.plot(pca.explained_variance_ratio_)

# components_
(pd.DataFrame(pca.components_, columns=activities_nonan.columns,
              index=[f'PC{i+1}' for i in range(activities_nonan.shape[1])])
.plot.bar()
)

"""Key components of PC1 are distance walking/running, resting and walking HR, flights climbed, steps and sleep. PC2 is composed of distance moved, move minutes, and calories burnt. This is also a very logical result.

# Clustering
"""

# Clustering

dend = hierarchy.dendrogram(hierarchy.linkage(std_X, method='ward'), 
                            truncate_mode='lastp', 
                            p=15,
                            #show_contracted = True
                            )

agg = cluster.AgglomerativeClustering(n_clusters=4,
                    affinity='euclidean', linkage='ward')
agg.fit(std_X)

""" The created diagram shows that there are 4 clusters of data"""

agg.labels_

(activities_nonan
 .assign(label=agg.labels_)
 .groupby('label')
 .agg(['median', 'std'])
 .T
 .iloc[4:]
 .style.background_gradient(cmap='RdBu', axis=1)
 )

(activities_nonan
 .assign(label=agg.labels_)
 .groupby('label')
 .mean()
 .T
 .plot.bar()
 )

"""Label 0 is composed of steps and distance walking/running. Label 1 is composed of steps, distance walking/running, and calories burnt. Label 2 is composed of steps, calories burnt, move minutes, and differences in distance. Label 3 is composed of steps, calories, and distance walking/ running.

# Conclusion

The PCA analysis confirmed initial assumptions that certain streams of data can be clustered together. Key components of PC1 are distance walking/running, resting and walking HR, flights climbed, and steps. This makes sense, as they seem to be closely related to each other, often changed during the same activity. Interestingly, also sleep plays an important role. PC2 is composed of distance moved, move minutes, and calories burnt. This is also a very logical result.

# Limitations 
After cleaning up my dataset properly, I realised that in my dataset the amount of missing values makes PCA a pretty unsuitable method for data analysis. However, I did it mostly to learn how to do it and interpret it. I filled in all of NaNs with 1 for the purpose of this exercise.

I found out about PPCA which seems to be a more suitable method for analysing this type of dataset, and I will be pursuing it after this course.
"""